# -*- coding: utf-8 -*-
"""HeartDiseasePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UriQHSWyC1imFP0RPOmpQNbfLK-oZvyM

# **Heart Disease Prediction**

Importing the libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from collections import Counter
import pickle

"""Loading the required csv file"""

# loading the required csv file
data = pd.read_csv('heart_disease_data.csv')



"""
### Partitioning the data

Splitting the features and target
"""

X = data.drop(columns='target', axis=1)
Y = data['target']

"""Splitting the dataset into training data and testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify = Y, random_state = 2)

"""## Model training

## Decision Tree Classifier
"""

from sklearn.tree import DecisionTreeClassifier

# Create a Decision Tree Classifier
model_dt = DecisionTreeClassifier()

# Fit the Decision Tree Classifier to the training data
model_dt.fit(X_train, Y_train)

# Make predictions on the test set using the Decision Tree Classifier
y_pred_dt = model_dt.predict(X_test)

if __name__ == '__main__':
    # Evaluate the accuracy of the Decision Tree Classifier
    accuracy_decision_tree = accuracy_score(Y_test, y_pred_dt)
    print("Decision Tree Classifier Accuracy:", accuracy_decision_tree)

    dt_confusion_matrix = confusion_matrix(Y_test, y_pred_dt)
    print("Decision Tree Classifier confusion matrix: ", dt_confusion_matrix)

"""## Random forest classifier"""

from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest Classifier
model_rf = RandomForestClassifier()

# Fit the Random Forest Classifier to the training data
model_rf.fit(X_train, Y_train)

# Make predictions on the test set using the Random Forest Classifier
y_pred_rf = model_rf.predict(X_test)

if __name__ == '__main__':
    # Evaluate the accuracy of the Random Forest Classifier
    accuracy_random_forest = accuracy_score(Y_test, y_pred_rf)
    print("Random Forest Classifier Accuracy:", accuracy_random_forest)

    rf_confusion_matrix = confusion_matrix(Y_test, y_pred_rf)
    print("Random Forest Classifier confusion matrix: ", rf_confusion_matrix)

"""## KNN"""

class KNN:
    def __init__(self, k):
        self.k = k

    def fit(self, X, Y):
        self.X_train = np.array(X)
        self.Y_train = np.array(Y)

    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2)**2))

    def predict(self, X):
        X = np.array(X)
        predictions = [self._predict(x) for x in X]
        return predictions

    def _predict(self, x):
        # compute the distance
        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]

        # get the closest k
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.Y_train[i] for i in k_indices]

        # majority vote
        most_common = Counter(k_nearest_labels).most_common()
        return most_common[0][0]

# fitting the data on model
model_knn = KNN(k=27)
model_knn.fit(X_train, Y_train)

if __name__ == '__main__':
    knn_predictions = model_knn.predict(X_test)

    knn_accuracy_score = accuracy_score(Y_test, knn_predictions)
    print("KNN accuracy score: ", knn_accuracy_score)

    knn_confusion_matrix = confusion_matrix(Y_test, knn_predictions)
    print("KNN confusion matrix: ", knn_confusion_matrix)

    """### Choosing the optimal k"""

    # Define a list of k values (odd numbers from 1 to 100)
    k_values = list(range(1, 41, 2))

    # Create lists to store accuracy scores
    accuracy_scores = []

    # Iterate over different k values
    for k in k_values:
        model_knn = KNN(k=k)
        model_knn.fit(X_train, Y_train)
        predictions = model_knn.predict(X_test)
        knn_accuracy_score = accuracy_score(Y_test, predictions)
        accuracy_scores.append(knn_accuracy_score)


    # finding k for which accuracy is maximum
    max_accuracy_index = np.argmax(accuracy_scores)

    k_max = k_values[max_accuracy_index]
    print("k for which accuracy is max: ",k_max)
    max_accuracy = accuracy_scores[max_accuracy_index]
    print("max accuracy: ",max_accuracy)

"""## Naive Bayes Classifier"""

class NaiveBayesClassifier:
    def __init__(self):
        self.class_priors = None
        self.class_likelihoods = None
        self.classes = None

    def fit(self, X, Y):
        self.classes = np.unique(Y)
        self.class_priors = {c: np.mean(Y == c) for c in self.classes}

        self.class_likelihoods = {}
        for c in self.classes:
            X_c = X[Y == c]
            class_mean = X_c.mean(axis=0)
            class_variance = X_c.var(axis=0)
            self.class_likelihoods[c] = {"mean": class_mean, "var": class_variance}

        return self

    def _pdf(self, x, mean, var):
        eps = 1e-8
        coeff = 1.0 / np.sqrt(2.0 * np.pi * var + eps)
        exponent = np.exp(-(np.square(x - mean) / (2.0 * var + eps)))
        return coeff * exponent

    def predict(self, X):
        if isinstance(X, pd.DataFrame):
            X = X.values

        predictions = []
        for i in range(len(X)):
            posterior_probs = {
                c: np.sum(np.log(self._pdf(X[i], self.class_likelihoods[c]["mean"], self.class_likelihoods[c]["var"]))) + np.log(self.class_priors[c])
                for c in self.classes
            }
            predicted_class = max(posterior_probs, key=posterior_probs.get)
            predictions.append(predicted_class)

        return np.array(predictions)

# fitting the data on model

model_nb = NaiveBayesClassifier()
model_nb.fit(X_train, Y_train)

if __name__ == '__main__':
    nb_predictions = model_nb.predict(X_test)

    nb_accuracy_score = accuracy_score(nb_predictions, Y_test)
    print("Naive Bayes Classifier accuracy score: ",nb_accuracy_score)

    nb_confusion_matrix = confusion_matrix(nb_predictions, Y_test)
    print("Naive Bayes Classifier confusion matrix: ",nb_confusion_matrix)

"""## Logistic regression"""

class Logistic_Regression():


  # declaring learning rate(Î±) & number of iterations
  def __init__(self, learning_rate, iterations):

    self.learning_rate = learning_rate
    self.iterations = iterations

  # fit function to train the model with dataset
  def fit(self, X, Y):

    # number of data points in the dataset (number of rows)  =  m
    # number of input features in the dataset (number of columns)  = n
    self.m, self.n = X.shape

    #initiating weight & bias value
    self.w = np.zeros(self.n)
    self.b = 0
    self.X = X
    self.Y = Y

    # implementing Gradient Descent for Optimization
    for i in range(self.iterations):
      self.update_weights()

  def update_weights(self):

    # Y_hat formula (sigmoid function)
    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))

    # derivaties
    dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))
    db = (1/self.m)*np.sum(Y_hat - self.Y)

    # updating the weights & bias using gradient descent
    self.w = self.w - self.learning_rate * dw
    self.b = self.b - self.learning_rate * db

  # Sigmoid Equation & Decision Boundary

  def predict(self, X):

    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) ))
    Y_pred = np.where( Y_pred > 0.5, 1, 0)
    return Y_pred

model_lg = Logistic_Regression(0.0001,100000)
#training the logistic regression model with training data
model_lg.fit(X_train, Y_train)

if __name__ == '__main__':
    
    lg_predictions = model_lg.predict(X_test)
    testing_data_accuracy = accuracy_score(lg_predictions, Y_test)

    lg_accuracy_score = accuracy_score(lg_predictions, Y_test)
    print("Logistic Regression accuracy score: ",lg_accuracy_score)

    lg_confusion_matrix = confusion_matrix(lg_predictions, Y_test)
    print("Logistic Regression confusion matrix: ",lg_confusion_matrix)

# Save the trained models to files using pickle
with open('model_dt.pkl', 'wb') as f_dt:
    pickle.dump(model_dt, f_dt)

with open('model_rf.pkl', 'wb') as f_rf:
    pickle.dump(model_rf, f_rf)

with open('model_knn.pkl', 'wb') as f_knn:
    pickle.dump(model_knn, f_knn)

with open('model_nb.pkl', 'wb') as f_nb:
    pickle.dump(model_nb, f_nb)

with open('model_lg.pkl', 'wb') as f_lg:
    pickle.dump(model_lg, f_lg)